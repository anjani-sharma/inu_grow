{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Parser Testing\n",
    "\n",
    "This notebook tests the enhanced CV parser with LLM integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import required modules\n",
    "from resume.resume_parser import extract_text\n",
    "from utils.cv_parser import CVParser, CVProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANJANI  SHARMA  \n",
      "Senior Data Science & AI Leader | Generative AI, Machine  Learning,  Python,  People leader \n",
      "Ph# 07436353049 , email: anjani.sharma1@gmail.com , LinkedIn:  LinkedIn    Project Repository: Github  \n",
      " \n",
      "Professional Summary  \n",
      "Accomplished data science leader with over 9 years of experience delivering production -grade machine learning solutions and leading high -\n",
      "performing teams. Proven track record in developing and deploying cutting -edge AI applications, including GenAI, recomm ...\n"
     ]
    }
   ],
   "source": [
    "# Path to test CV\n",
    "cv_path = '../resume/Anjani Sharma.pdf'\n",
    "\n",
    "# Extract text from the CV\n",
    "cv_text = extract_text(cv_path)\n",
    "\n",
    "# Print the first 500 characters of the CV text\n",
    "print(cv_text[:500] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Rule-Based Extraction\n",
    "\n",
    "First, let's test just the regex-based extraction to see what information we can get without LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting personal info with LLM: 'github'\n",
      "name: ANJANI SHARMA\n",
      "position: Senior Data Science & AI Leader\n",
      "email: anjani.sharma1@gmail.com\n",
      "phone: 07436353049\n",
      "linkedin: LinkedIn\n",
      "website: \n",
      "location: Brighton, UK\n"
     ]
    }
   ],
   "source": [
    "# Create a parser instance\n",
    "parser = CVParser(cv_text)\n",
    "\n",
    "# Extract basic info using regex patterns\n",
    "parser._extract_personal_info()\n",
    "\n",
    "# Print the extracted personal information\n",
    "for key in ['name', 'position', 'email', 'phone', 'linkedin', 'website', 'location']:\n",
    "    print(f\"{key}: {parser.result.get(key, 'Not found')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified Sections:\n",
      "- summary\n",
      "- skills\n",
      "- experience\n",
      "- projects\n",
      "- education\n",
      "- certifications\n"
     ]
    }
   ],
   "source": [
    "# Identify sections in the CV\n",
    "sections = parser._identify_sections()\n",
    "\n",
    "# Print the identified sections\n",
    "print(\"Identified Sections:\")\n",
    "for section_name in sections.keys():\n",
    "    print(f\"- {section_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LLM-Enhanced Parsing\n",
    "\n",
    "Now, let's test the full parser with LLM integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting personal info with LLM: 'github'\n",
      "\n",
      "--- BASIC INFO ---\n",
      "name: ANJANI SHARMA\n",
      "position: Senior Data Science & AI Leader\n",
      "email: anjani.sharma1@gmail.com\n",
      "phone: 07436353049\n",
      "linkedin: LinkedIn profile URL if found\n",
      "website: \n",
      "location: Brighton, UK\n"
     ]
    }
   ],
   "source": [
    "# Create a profile instance (uses the full parser)\n",
    "profile = CVProfile(cv_text)\n",
    "\n",
    "# Get the parsed data\n",
    "parsed_data = profile.parsed_data\n",
    "\n",
    "# Print basic personal information\n",
    "print(\"\\n--- BASIC INFO ---\")\n",
    "for key in ['name', 'position', 'email', 'phone', 'linkedin', 'website', 'location']:\n",
    "    print(f\"{key}: {parsed_data.get(key, 'Not found')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SKILLS ---\n",
      "- Python (Programming Languages)\n",
      "- SQL (Programming Languages)\n",
      "- Java (Programming Languages)\n",
      "- R (Programming Languages)\n",
      "- Generative AI (Data Science & Analytics)\n",
      "- Machine Learning & AI (Data Science & Analytics)\n",
      "- Big Data Tools (Data Science & Analytics)\n",
      "- Data Visualization Tools (Data Science & Analytics)\n",
      "- Cloud Platforms (DevOps & Cloud)\n",
      "- Version Control (Tools & Software)\n",
      "... and 1 more skills\n"
     ]
    }
   ],
   "source": [
    "# Print skills (first 10)\n",
    "print(\"\\n--- SKILLS ---\")\n",
    "skills = parsed_data.get('skills', [])\n",
    "for skill in skills[:10]:\n",
    "    print(f\"- {skill.get('name', 'Unknown')} ({skill.get('category', 'Unknown')})\") \n",
    "if len(skills) > 10:\n",
    "    print(f\"... and {len(skills) - 10} more skills\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- WORK EXPERIENCE ---\n",
      "- Founder & Head of Data Science and AI at AI Transformers Ltd\n",
      "  Apr '2023 to Present\n",
      "  Achievements:\n",
      "  - Improved customer retention by 15% through ML-driven segmentation\n",
      "  - Increased marketing campaign conversion by 20% using advanced ML models\n",
      "  ... and 1 more achievements\n",
      "\n",
      "- Sr. Data Scientist at Admiral Group Plc\n",
      "  Aug '2023 to Mar '2024\n",
      "\n",
      "- Lead Data Scientist at Mindmap Consulting Digital\n",
      "  June '2020 to Jul '2023\n",
      "\n",
      "- Analytics Lead at Mindmap Consulting Digital\n",
      "  June '2016 to May '2020\n",
      "\n",
      "- Freelance Data Scientist at Mindmap Consulting Digital\n",
      "  Nov '2014 to June '2016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print work experience\n",
    "print(\"\\n--- WORK EXPERIENCE ---\")\n",
    "experiences = parsed_data.get('work_experience', [])\n",
    "for exp in experiences:\n",
    "    print(f\"- {exp.get('title', 'Unknown')} at {exp.get('company', 'Unknown')}\")\n",
    "    print(f\"  {exp.get('start_date', '')} to {exp.get('end_date', '')}\")\n",
    "    \n",
    "    # Print a few achievements if available\n",
    "    achievements = exp.get('achievements', [])\n",
    "    if achievements:\n",
    "        print(\"  Achievements:\")\n",
    "        for achievement in achievements[:2]:\n",
    "            print(f\"  - {achievement}\")\n",
    "        if len(achievements) > 2:\n",
    "            print(f\"  ... and {len(achievements) - 2} more achievements\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EDUCATION ---\n",
      "- BA - Honours in Economics and Mathematics\n",
      "  St. Josephâ€™s College, North Bengal University\n",
      "  January 2000 to January 2003\n",
      "\n",
      "- Data Science Specialization in \n",
      "  \n",
      "  January 2023 to January 2025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print education\n",
    "print(\"\\n--- EDUCATION ---\")\n",
    "education = parsed_data.get('education', [])\n",
    "for edu in education:\n",
    "    print(f\"- {edu.get('degree', 'Unknown')} in {edu.get('field', 'Unknown')}\")\n",
    "    print(f\"  {edu.get('institution', 'Unknown')}\")\n",
    "    print(f\"  {edu.get('start_date', '')} to {edu.get('end_date', '')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PROJECTS ---\n",
      "- Customer Support Chatbot\n",
      "  Developed and deployed a chatbot for customer support using LangChain and OpenAI API.\n",
      "  Technologies: LangChain, OpenAI, Python, AWS\n",
      "\n",
      "- Text Classification Project\n",
      "  Conducted multi-class text classification using NLTK and BERT for text analysis.\n",
      "  Technologies: NLTK, BERT\n",
      "\n",
      "- Metadata Generation Project\n",
      "  Implemented a metadata generation project using LLaMA2, FAISS, and RAGs.\n",
      "  Technologies: LLaMA2, FAISS, RAGs, Python\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print projects (if available)\n",
    "print(\"\\n--- PROJECTS ---\")\n",
    "projects = parsed_data.get('projects', [])\n",
    "for project in projects[:3]:\n",
    "    print(f\"- {project.get('name', 'Unknown')}\")\n",
    "    print(f\"  {project.get('description', '')}\")\n",
    "    \n",
    "    # Print technologies if available\n",
    "    technologies = project.get('technologies', [])\n",
    "    if technologies:\n",
    "        print(f\"  Technologies: {', '.join(technologies[:5])}\")\n",
    "        if len(technologies) > 5:\n",
    "            print(f\"  ... and {len(technologies) - 5} more technologies\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Skill Enhancement\n",
    "\n",
    "Let's examine the enhanced skills feature that adds related skills based on the parsed skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Skills:\n",
      "python, sql, java, r, generative ai, machine learning & ai, big data tools, data visualization tools, cloud platforms, version control\n",
      "... and 1 more skills\n",
      "\n",
      "Enhanced Skills:\n",
      "neural networks, java, r, generative ai, cloud platforms, python, deep learning, sql, statistics, version control\n",
      "... and 11 more skills\n",
      "\n",
      "Newly Added Skills:\n",
      "neural networks, deep learning, statistics, predictive modeling, data analysis, natural language processing, ETL, data science, data engineering, data warehousing\n"
     ]
    }
   ],
   "source": [
    "# Get original skills (names only)\n",
    "original_skills = [s['name'].strip().lower() for s in parsed_data.get('skills', []) if s]\n",
    "print(\"Original Skills:\")\n",
    "print(\", \".join(original_skills[:10]))\n",
    "if len(original_skills) > 10:\n",
    "    print(f\"... and {len(original_skills) - 10} more skills\")\n",
    "\n",
    "# Get enhanced skills\n",
    "enhanced_skills = profile.skills\n",
    "print(\"\\nEnhanced Skills:\")\n",
    "print(\", \".join(enhanced_skills[:10]))\n",
    "if len(enhanced_skills) > 10:\n",
    "    print(f\"... and {len(enhanced_skills) - 10} more skills\")\n",
    "\n",
    "# Find new skills added through enhancement\n",
    "new_skills = [s for s in enhanced_skills if s.lower() not in original_skills]\n",
    "print(\"\\nNewly Added Skills:\")\n",
    "print(\", \".join(new_skills))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Parsed Data\n",
    "\n",
    "Finally, let's see the complete parsed data in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type datetime is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Convert the parsed data to a formatted JSON string\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m parsed_json = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Print the JSON\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(parsed_json)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/grow/lib/python3.13/json/__init__.py:238\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/grow/lib/python3.13/json/encoder.py:200\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    202\u001b[39m     chunks = \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/grow/lib/python3.13/json/encoder.py:261\u001b[39m, in \u001b[36mJSONEncoder.iterencode\u001b[39m\u001b[34m(self, o, _one_shot)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     _iterencode = _make_iterencode(\n\u001b[32m    258\u001b[39m         markers, \u001b[38;5;28mself\u001b[39m.default, _encoder, indent, floatstr,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mself\u001b[39m.key_separator, \u001b[38;5;28mself\u001b[39m.item_separator, \u001b[38;5;28mself\u001b[39m.sort_keys,\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m.skipkeys, _one_shot)\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/grow/lib/python3.13/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type datetime is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# Convert the parsed data to a formatted JSON string\n",
    "parsed_json = json.dumps(parsed_data, indent=2)\n",
    "\n",
    "# Print the JSON\n",
    "print(parsed_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
